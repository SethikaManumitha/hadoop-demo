version: '3.8'

services:
  # -----------------------------
  # HDFS - NameNode (Hadoop 2.7.4)
  # -----------------------------
  namenode:
    image: ramilu90/hive-namenode:1.0.0
    container_name: namenode
    restart: always
    env_file:
      - ./hadoop.env
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_NAMENODE_RPC_ADDRESS=namenode:8020
    volumes:
      - /home/iitgcpuser/iit/lab5/hadoop-hive-dockercompose/recharge2.input:/opt/recharge2.input
      - ./data_new/namenode:/hadoop/dfs/name
      - /home/iitgcpuser/hadoop-demo/hadoop-setup/hadoop_dir:/opt/hadoop/etc/hadoop

    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - hadoop-net
    healthcheck:
      test: [ "CMD", "hdfs", "dfsadmin", "-report" ]
      interval: 10s
      timeout: 5s
      retries: 20

  # -----------------------------
  # HDFS - DataNode (Hadoop 2.7.4)
  # -----------------------------
  datanode:
    image: ramilu90/hive-datanode:1.0.0
    container_name: datanode
    restart: always
    env_file:
      - ./hadoop.env
    environment:
      - SERVICE_PRECONDITION=namenode:8020
      - HDFS_NAMENODE_RPC_ADDRESS=namenode:8020
    volumes:
      - ./data_new/datanode:/hadoop/dfs/data
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop-net

  # -----------------------------
  # PostgreSQL for Hive Metastore
  # -----------------------------
  hive-metastore-postgresql:
    image: ramilu90/hive-metastore-postgresql:1.0.0
    container_name: hive-metastore-postgresql
    restart: always
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    volumes:
      - metastore-db2:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - hadoop-net
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U hive" ]
      interval: 5s
      retries: 20
      timeout: 5s

  # -----------------------------
  # Hive Metastore Schema Initialization
  # -----------------------------
  hive-metastore-init:
    image: ramilu90/hive-metastore-init:1.0.0
    container_name: hive-metastore-init
    env_file:
      - ./hadoop.env
    depends_on:
      hive-metastore-postgresql:
        condition: service_healthy
      namenode:
        condition: service_healthy
    command: [ "schematool", "-dbType", "postgres", "-initSchema", "-verbose" ]
    networks:
      - hadoop-net


  hive-metastore:
    image: ramilu90/hive-metastore:1.0.0
    container_name: hive-metastore
    env_file:
      - ./hadoop.env
    environment: # Added for HDFS client stability
      - HADOOP_USER_NAME=root
    depends_on:
      hive-metastore-init:
        condition: service_completed_successfully
      namenode:
        condition: service_healthy
    command: /opt/hive/bin/hive --service metastore # <-- Simplified command
    ports:
      - "9083:9083"
    networks:
      - hadoop-net


  # -----------------------------
  # HiveServer2
  # -----------------------------
    # -----------------------------
    # HiveServer2
    # -----------------------------
  hive-server: # <-- ENSURE THIS IS ALIGNED WITH 'hive-metastore'
    image: ramilu90/hive-server:1.0.0
    container_name: hive-server
    platform: linux/amd64
    volumes:
      - /home/iitgcpuser/iit/lab5/hadoop-hive-dockercompose/recharge2.input:/opt/recharge2.input
    env_file:
      - ./hadoop.env
    environment: # Added for HDFS client stability
      - HADOOP_USER_NAME=root
    depends_on:
      hive-metastore:
        condition: service_started
      namenode:
        condition: service_healthy
    command: /opt/hive/bin/hiveserver2 # <-- Simplified command
    ports:
      - "10000:10000"
    networks:
      - hadoop-net

  spark-master:
    image: ramilu90/spark-master
    container_name: spark-master
    platform: linux/amd64
    volumes:
      - /home/iitgcpuser/iit/lab7/hadoop-spark-dockercompose/resources/:/opt/resources
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks:
      - hadoop-net

  spark-worker-1:
    image: ramilu90/spark-worker
    container_name: spark-worker-1
    platform: linux/amd64
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    networks:
      - hadoop-net

zepplin:
  image: apache/zeppelin:0.9.0-spark-2.4.8
  container_name: zeppelin
  ports:
    - "8089:8080"
  environment:
    - SPARK_MASTER=spark://spark-master:7077
    - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
    - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  volumes:
    - /home/iitgcpuser/hadoop-demo/hadoop-setup/hadoop_dir:/opt/hadoop/etc/hadoop
  depends_on:
    - spark-master
    - namenode
    - datanode
  networks:
    - hadoop-net



volumes:
  metastore-db2:

networks:
  hadoop-net:
    driver: bridge