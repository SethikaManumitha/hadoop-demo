{
  "metadata": {
    "name": "spark_task",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nfrom pyspark.sql.functions import *\r\nfrom pyspark.sql import SparkSession\r\n\r\nspark \u003d SparkSession.builder.appName(\"Task2Spark\").getOrCreate()\r\n\r\nlocations \u003d spark.read.csv(\"hdfs://namenode:8020/user/data/location/processed_location_data.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\nweather \u003d spark.read.csv(\"hdfs://namenode:8020/user/data/weather/processed_weather_data.csv\", header\u003dTrue, inferSchema\u003dTrue)\r\n\r\nweather \u003d weather.withColumn(\"date\", to_date(col(\"date\")))\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\n# Add month column\r\nweather_month \u003d weather.withColumn(\"month\", month(col(\"date\")))\r\n\r\nweather_month.show(2)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Total rows per month\nmonthly_total \u003d weather_month.groupBy(\"month\").agg(count(\"*\").alias(\"total_count\"))\n\n# Rows above 15 \nmonthly_above \u003d weather_month.filter(col(\"shortwave_radiation_sum\") \u003e 15).groupBy(\"month\").agg(count(\"*\").alias(\"above_15_count\"))\nradiation_percentage \u003d monthly_total.join(monthly_above, \"month\", \"left\") .withColumn(\"percentage_above_15\",(col(\"above_15_count\") / col(\"total_count\")) * 100).orderBy(\"month\")\nradiation_percentage.show() # Show percentage of radiation above 15"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nweather2 \u003d weather.withColumn(\"year\", year(col(\"date\"))).withColumn(\"month\", month(col(\"date\"))).withColumn(\"week\", weekofyear(col(\"date\")))\n\n# compute average temperature per month per year\nmonthly_avg \u003d weather2.groupBy(\"year\", \"month\").agg(avg(\"temperature_2m_mean\").alias(\"avg_temp\"))\nmonthly_avg.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom pyspark.sql.window import Window\nw \u003d Window.partitionBy(\"year\").orderBy(col(\"avg_temp\").desc())\n\nhottest_months \u003d monthly_avg.withColumn(\"rank\", rank().over(w)).filter(col(\"rank\") \u003d\u003d 1).select(\"year\", \"month\")\n    \nhottest_months.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfiltered_weather \u003d weather2.join(hottest_months, [\"year\", \"month\"])\nweekly_max_temp \u003d filtered_weather.groupBy(\"year\", \"month\", \"week\").agg(max(\"temperature_2m_max\").alias(\"weekly_max_temp\")).orderBy(\"year\", \"week\")\nweekly_max_temp.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}